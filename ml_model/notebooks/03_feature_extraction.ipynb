{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f601b7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "âœ… **Feature Extraction Complete!**\n",
    "\n",
    "**What was generated:**\n",
    "1. **Sentence Embeddings** (384-dimensional)\n",
    "   - Using all-MiniLM-L6-v2 model\n",
    "   - Fast & efficient pre-trained model\n",
    "   - Saved for train/test/val sets\n",
    "\n",
    "2. **TF-IDF Features** (5000 features)\n",
    "   - Unigrams + Bigrams\n",
    "   - English stopwords removed\n",
    "   - Sublinear TF scaling\n",
    "\n",
    "3. **Text Features** (7 features)\n",
    "   - Claim length, word count\n",
    "   - Unique words, word diversity\n",
    "   - Uppercase letters, digits, sentences\n",
    "\n",
    "4. **Combined Feature Datasets**\n",
    "   - 5392+ total features per sample\n",
    "   - Ready for ML model training\n",
    "\n",
    "**Saved Files:**\n",
    "- `models/sentence_embedding_model/` - Embedding model\n",
    "- `models/train_embeddings.npy` - Training embeddings\n",
    "- `models/test_embeddings.npy` - Test embeddings\n",
    "- `models/val_embeddings.npy` - Validation embeddings\n",
    "- `models/tfidf_vectorizer.pkl` - TF-IDF vectorizer\n",
    "- `preprocessed_data/train_features.csv` - Combined features\n",
    "- `preprocessed_data/test_features.csv` - Combined features\n",
    "- `preprocessed_data/val_features.csv` - Combined features\n",
    "- `models/feature_metadata.json` - Feature metadata\n",
    "\n",
    "**Next Steps:**\n",
    "- Notebook 04: Semantic Similarity Model\n",
    "- Notebook 05: NLI Model Training ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a9160f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saving models and metadata...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'embedding_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2055527061.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Save embedding model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0membedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./models/sentence_embedding_model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"   âœ… Embedding model saved\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embedding_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Save embedding model and metadata\n",
    "import json\n",
    "\n",
    "print(\"ðŸ’¾ Saving models and metadata...\")\n",
    "\n",
    "# Save embedding model\n",
    "embedding_model.save('./models/sentence_embedding_model')\n",
    "print(\"   âœ… Embedding model saved\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'embedding_model': model_name,\n",
    "    'embedding_dimension': embedding_model.get_sentence_embedding_dimension(),\n",
    "    'tfidf_max_features': 5000,\n",
    "    'tfidf_ngram_range': [1, 2],\n",
    "    'train_samples': len(train_embeddings),\n",
    "    'test_samples': len(test_embeddings),\n",
    "    'val_samples': len(val_embeddings),\n",
    "    'feature_description': {\n",
    "        'embeddings': 'Pre-trained sentence embeddings (384-dim)',\n",
    "        'tfidf': 'TF-IDF vectorization (5000 features)',\n",
    "        'claim_length': 'Length of claim in characters',\n",
    "        'claim_word_count': 'Number of words in claim',\n",
    "        'unique_words': 'Unique word count',\n",
    "        'word_diversity': 'Ratio of unique words to total words',\n",
    "        'num_uppercase': 'Count of uppercase letters',\n",
    "        'num_digits': 'Count of digits',\n",
    "        'num_sentences': 'Estimated number of sentences'\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('./models/feature_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "\n",
    "print(\"   âœ… Metadata saved\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE EXTRACTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nâœ… Embeddings Generated:\")\n",
    "print(f\"   - Model: {model_name}\")\n",
    "print(f\"   - Dimension: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"   - Train: {train_embeddings.shape}\")\n",
    "print(f\"   - Test: {test_embeddings.shape}\")\n",
    "print(f\"   - Val: {val_embeddings.shape}\")\n",
    "\n",
    "print(f\"\\nâœ… Additional Features:\")\n",
    "print(f\"   - TF-IDF vectors (5000 features)\")\n",
    "print(f\"   - Text statistics (7 features)\")\n",
    "print(f\"   - Total feature columns: {train_feature_df.shape[1]}\")\n",
    "\n",
    "print(f\"\\nâœ… Models Saved:\")\n",
    "print(f\"   - Sentence Transformers model\")\n",
    "print(f\"   - TF-IDF vectorizer\")\n",
    "print(f\"   - Embedding arrays (.npy files)\")\n",
    "print(f\"   - Feature datasets (.csv files)\")\n",
    "print(f\"   - Metadata (.json file)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1bb865",
   "metadata": {},
   "source": [
    "## Step 8: Save Model & Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7300a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature correlations\n",
    "print(\"ðŸ“Š Feature Correlation Analysis...\")\n",
    "\n",
    "# Select non-embedding features for analysis\n",
    "text_features_cols = ['claim_length', 'claim_word_count', 'unique_words', 'word_diversity',\n",
    "                      'num_uppercase', 'num_digits', 'num_sentences']\n",
    "\n",
    "text_features_train = train_feature_df[text_features_cols + ['label']].copy()\n",
    "\n",
    "# Convert labels to numeric for correlation\n",
    "label_to_numeric = {'REAL': 0, 'FAKE': 1, 'NOT_ENOUGH_INFO': 2}\n",
    "text_features_train['label_numeric'] = text_features_train['label'].map(label_to_numeric)\n",
    "\n",
    "# Calculate correlation\n",
    "correlation_matrix = text_features_train[text_features_cols + ['label_numeric']].corr()\n",
    "\n",
    "# Plot correlation\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(correlation_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "\n",
    "ax.set_xticks(range(len(correlation_matrix.columns)))\n",
    "ax.set_yticks(range(len(correlation_matrix.columns)))\n",
    "ax.set_xticklabels(correlation_matrix.columns, rotation=45, ha='right')\n",
    "ax.set_yticklabels(correlation_matrix.columns)\n",
    "\n",
    "# Add values to heatmap\n",
    "for i in range(len(correlation_matrix)):\n",
    "    for j in range(len(correlation_matrix)):\n",
    "        text = ax.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='Correlation')\n",
    "ax.set_title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Feature correlation analyzed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10226f56",
   "metadata": {},
   "source": [
    "## Step 7: Feature Correlation & Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb44c90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrices with additional features\n",
    "def create_feature_dataset(df, tfidf_matrix, embeddings, label_col='label'):\n",
    "    \"\"\"Combine all features into one dataset\"\"\"\n",
    "    \n",
    "    # Convert TF-IDF to dense array\n",
    "    tfidf_dense = tfidf_matrix.toarray()\n",
    "    \n",
    "    # Create a dataframe with all features\n",
    "    feature_df = pd.DataFrame(embeddings, columns=[f'embedding_{i}' for i in range(embeddings.shape[1])])\n",
    "    \n",
    "    # Add TF-IDF features\n",
    "    tfidf_feature_names = [f'tfidf_{i}' for i in range(tfidf_dense.shape[1])]\n",
    "    feature_df[tfidf_feature_names] = pd.DataFrame(tfidf_dense)\n",
    "    \n",
    "    # Add basic text features\n",
    "    text_features = add_text_features(df)\n",
    "    feature_cols = ['claim_length', 'claim_word_count', 'unique_words', 'word_diversity',\n",
    "                    'num_uppercase', 'num_digits', 'num_sentences']\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        feature_df[col] = text_features[col].values\n",
    "    \n",
    "    # Add label and metadata\n",
    "    feature_df['claim_id'] = df['claim_id'].values\n",
    "    feature_df['label'] = df['label'].values\n",
    "    feature_df['claim'] = df['claim'].values\n",
    "    \n",
    "    return feature_df\n",
    "\n",
    "print(\"ðŸ“¦ Creating combined feature datasets...\")\n",
    "train_feature_df = create_feature_dataset(train_df, train_tfidf, train_embeddings)\n",
    "test_feature_df = create_feature_dataset(test_df, test_tfidf, test_embeddings)\n",
    "val_feature_df = create_feature_dataset(val_df, val_tfidf, val_embeddings)\n",
    "\n",
    "print(f\"âœ… Feature datasets created!\")\n",
    "print(f\"   Train shape: {train_feature_df.shape}\")\n",
    "print(f\"   Test shape: {test_feature_df.shape}\")\n",
    "print(f\"   Val shape: {val_feature_df.shape}\")\n",
    "\n",
    "# Save feature datasets\n",
    "train_feature_df.to_csv('./preprocessed_data/train_features.csv', index=False)\n",
    "test_feature_df.to_csv('./preprocessed_data/test_features.csv', index=False)\n",
    "val_feature_df.to_csv('./preprocessed_data/val_features.csv', index=False)\n",
    "\n",
    "print(\"\\nâœ… Feature datasets saved to ./preprocessed_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d170a0c3",
   "metadata": {},
   "source": [
    "## Step 6: Create Combined Feature Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510e3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze embeddings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"ðŸ“Š Embedding Statistics (Training set):\")\n",
    "print(f\"   Mean norm: {np.linalg.norm(train_embeddings, axis=1).mean():.4f}\")\n",
    "print(f\"   Std norm: {np.linalg.norm(train_embeddings, axis=1).std():.4f}\")\n",
    "print(f\"   Min mean value: {train_embeddings.mean(axis=0).min():.4f}\")\n",
    "print(f\"   Max mean value: {train_embeddings.mean(axis=0).max():.4f}\")\n",
    "\n",
    "# Dimensionality reduction for visualization\n",
    "print(\"\\nðŸŽ¨ Reducing dimensions for visualization...\")\n",
    "pca = PCA(n_components=2)\n",
    "train_embeddings_2d = pca.fit_transform(train_embeddings[:5000])  # Use subset for speed\n",
    "\n",
    "print(f\"   Explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Plot embeddings by label\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "labels_unique = train_df['label'].unique()\n",
    "colors = {'REAL': 'green', 'FAKE': 'red', 'NOT_ENOUGH_INFO': 'orange'}\n",
    "\n",
    "for label in labels_unique:\n",
    "    indices = train_df.iloc[:5000]['label'] == label\n",
    "    ax.scatter(\n",
    "        train_embeddings_2d[indices, 0],\n",
    "        train_embeddings_2d[indices, 1],\n",
    "        c=colors.get(label, 'blue'),\n",
    "        label=label,\n",
    "        alpha=0.6,\n",
    "        s=30\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_title('Embedding Space Visualization (PCA 2D)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbca4a6",
   "metadata": {},
   "source": [
    "## Step 5: Embedding Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e146cf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Transformers for semantic embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"ðŸ§  Loading pre-trained sentence transformer model...\")\n",
    "# Using a lightweight model (better for inference speed)\n",
    "model_name = 'all-MiniLM-L6-v2'  # Fast and efficient (~22MB)\n",
    "# Alternative: 'all-mpnet-base-v2' (better quality, larger)\n",
    "embedding_model = SentenceTransformer(model_name)\n",
    "\n",
    "print(f\"   Model: {model_name}\")\n",
    "print(f\"   Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# Generate embeddings\n",
    "print(\"\\nðŸ“¥ Generating embeddings for training set...\")\n",
    "train_embeddings = embedding_model.encode(\n",
    "    train_df['claim'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "print(\"ðŸ“¥ Generating embeddings for test set...\")\n",
    "test_embeddings = embedding_model.encode(\n",
    "    test_df['claim'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "print(\"ðŸ“¥ Generating embeddings for val set...\")\n",
    "val_embeddings = embedding_model.encode(\n",
    "    val_df['claim'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Embeddings generated!\")\n",
    "print(f\"   Train embeddings shape: {train_embeddings.shape}\")\n",
    "print(f\"   Test embeddings shape: {test_embeddings.shape}\")\n",
    "print(f\"   Val embeddings shape: {val_embeddings.shape}\")\n",
    "\n",
    "# Save embeddings\n",
    "np.save('./models/train_embeddings.npy', train_embeddings)\n",
    "np.save('./models/test_embeddings.npy', test_embeddings)\n",
    "np.save('./models/val_embeddings.npy', val_embeddings)\n",
    "\n",
    "print(\"\\nâœ… Embeddings saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dc36cb",
   "metadata": {},
   "source": [
    "## Step 4: Sentence Embeddings (Pre-trained Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fca7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "print(\"ðŸ”¤ Creating TF-IDF vectorizer...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,           # Limit to top 5000 features\n",
    "    min_df=2,                    # Minimum document frequency\n",
    "    max_df=0.8,                  # Maximum document frequency\n",
    "    ngram_range=(1, 2),          # Unigrams and bigrams\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "print(\"  Fitting on training data...\")\n",
    "train_tfidf = tfidf_vectorizer.fit_transform(train_df['claim'])\n",
    "\n",
    "# Transform test and val data\n",
    "print(\"  Transforming test and val data...\")\n",
    "test_tfidf = tfidf_vectorizer.transform(test_df['claim'])\n",
    "val_tfidf = tfidf_vectorizer.transform(val_df['claim'])\n",
    "\n",
    "print(f\"\\nâœ… TF-IDF vectorization complete!\")\n",
    "print(f\"   Train TF-IDF shape: {train_tfidf.shape}\")\n",
    "print(f\"   Test TF-IDF shape: {test_tfidf.shape}\")\n",
    "print(f\"   Val TF-IDF shape: {val_tfidf.shape}\")\n",
    "print(f\"   Vocabulary size: {len(tfidf_vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "# Save TF-IDF vectorizer\n",
    "import pickle\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "pickle.dump(tfidf_vectorizer, open('./models/tfidf_vectorizer.pkl', 'wb'))\n",
    "print(\"\\nâœ… TF-IDF vectorizer saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c500e4",
   "metadata": {},
   "source": [
    "## Step 3: TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ab19af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering functions\n",
    "def add_text_features(df):\n",
    "    \"\"\"Add basic text features\"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Length features\n",
    "    df_features['claim_length'] = df_features['claim'].str.len()\n",
    "    df_features['claim_word_count'] = df_features['claim'].str.split().str.len()\n",
    "    \n",
    "    # Vocabulary features\n",
    "    df_features['unique_words'] = df_features['claim'].apply(lambda x: len(set(x.split())))\n",
    "    df_features['word_diversity'] = df_features['unique_words'] / (df_features['claim_word_count'] + 1)\n",
    "    \n",
    "    # Punctuation & special characters\n",
    "    df_features['num_uppercase'] = df_features['claim'].apply(lambda x: sum(1 for c in x if c.isupper()))\n",
    "    df_features['num_digits'] = df_features['claim'].apply(lambda x: sum(1 for c in x if c.isdigit()))\n",
    "    df_features['num_sentences'] = df_features['claim'].apply(lambda x: x.count('.') + x.count('!') + x.count('?'))\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\"ðŸ”§ Adding text features...\")\n",
    "train_features = add_text_features(train_df)\n",
    "test_features = add_text_features(test_df)\n",
    "val_features = add_text_features(val_df)\n",
    "\n",
    "print(\"âœ… Text features added!\")\n",
    "print(\"\\nðŸ“Š Feature statistics (Training set):\")\n",
    "print(train_features[['claim_length', 'claim_word_count', 'unique_words', 'word_diversity']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135e200f",
   "metadata": {},
   "source": [
    "## Step 2: Text Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e4e548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "train_df = pd.read_csv('./preprocessed_data/train_data.csv')\n",
    "test_df = pd.read_csv('./preprocessed_data/test_data.csv')\n",
    "val_df = pd.read_csv('./preprocessed_data/val_data.csv')\n",
    "\n",
    "print(\"âœ… Data loaded!\")\n",
    "print(f\"Train: {train_df.shape}\")\n",
    "print(f\"Test: {test_df.shape}\")\n",
    "print(f\"Val: {val_df.shape}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Sample data:\")\n",
    "print(train_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2b3b83",
   "metadata": {},
   "source": [
    "## Step 1: Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13f315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ce5b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for embeddings\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = ['pandas', 'numpy', 'scikit-learn', 'nltk', 'gensim', 'sentence-transformers']\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package if package != 'gensim' else 'gensim.models')\n",
    "        print(f\"âœ… {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"ðŸ“¦ Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"âœ… {package} installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd64ec2",
   "metadata": {},
   "source": [
    "# 03 - Feature Extraction & Text Embeddings\n",
    "## Generate Embeddings for Claim Verification\n",
    "\n",
    "This notebook creates text embeddings and features from preprocessed claims for semantic similarity and NLI models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
