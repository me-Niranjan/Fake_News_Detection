{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdb33a53",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "‚úÖ **LIAR Dataset Overview:**\n",
    "- **Total Claims**: ~12,836 labeled claims\n",
    "- **Labels**: 6 categories (true, mostly-true, half-true, false, barely-true, pants-fire)\n",
    "- **Features**: Speaker, context, job title, party affiliation, and counts of different labels\n",
    "- **Task**: Convert 6 labels ‚Üí 3 labels (REAL, FAKE, NOT_ENOUGH_INFO)\n",
    "\n",
    "**Next Step**: Run Notebook 02 for Data Preprocessing & Label Conversion üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e3bc8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SAMPLE CLAIMS BY LABEL\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1918768093.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nüìå Label: {label.upper()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'claim'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Display sample claims by label\n",
    "print(\"=\" * 60)\n",
    "print(\"SAMPLE CLAIMS BY LABEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for label in train_df['label'].unique():\n",
    "    print(f\"\\nüìå Label: {label.upper()}\")\n",
    "    sample = train_df[train_df['label'] == label]['claim'].iloc[0]\n",
    "    print(f\"   {sample[:100]}...\")\n",
    "    print(f\"   Speaker: {train_df[train_df['label'] == label]['speaker'].iloc[0]}\")\n",
    "\n",
    "print(\"\\n‚úÖ Sample exploration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96427a34",
   "metadata": {},
   "source": [
    "## Step 6: Sample Claims Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa37bb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"=\" * 60)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüîç Train set missing values:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "print(\"\\nüîç Test set missing values:\")\n",
    "print(test_df.isnull().sum())\n",
    "\n",
    "print(\"\\nüîç Validation set missing values:\")\n",
    "print(val_df.isnull().sum())\n",
    "\n",
    "# Visualize missing values\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "missing_counts = train_df.isnull().sum()\n",
    "missing_counts = missing_counts[missing_counts > 0]\n",
    "if len(missing_counts) > 0:\n",
    "    missing_counts.plot(kind='barh', color='red', ax=ax)\n",
    "    ax.set_title('Missing Values in Training Set')\n",
    "else:\n",
    "    ax.text(0.5, 0.5, 'No missing values!', ha='center', va='center', fontsize=14)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Missing values analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d350ac",
   "metadata": {},
   "source": [
    "## Step 5: Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3700819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Add text length columns\n",
    "for df in [train_df, test_df, val_df]:\n",
    "    df['claim_length'] = df['claim'].str.len()\n",
    "    df['claim_word_count'] = df['claim'].str.split().str.len()\n",
    "\n",
    "print(\"\\nüìù Claim Length Statistics (Training set):\")\n",
    "print(train_df['claim_length'].describe())\n",
    "\n",
    "print(\"\\nüìù Claim Word Count Statistics (Training set):\")\n",
    "print(train_df['claim_word_count'].describe())\n",
    "\n",
    "# Plot text length distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "axes[0].hist(train_df['claim_length'], bins=50, color='coral', edgecolor='black')\n",
    "axes[0].set_title('Distribution of Claim Length (characters)')\n",
    "axes[0].set_xlabel('Length')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].hist(train_df['claim_word_count'], bins=50, color='lightgreen', edgecolor='black')\n",
    "axes[1].set_title('Distribution of Claim Word Count')\n",
    "axes[1].set_xlabel('Word Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Text statistics analyzed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c712e8",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Text Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490eaa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Distribution\n",
    "print(\"=\" * 60)\n",
    "print(\"LABEL DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "labels = ['mostly-true', 'true', 'half-true', 'false', 'barely-true', 'pants-fire']\n",
    "\n",
    "print(\"\\nüìä Train set labels:\")\n",
    "print(train_df['label'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nüìä Test set labels:\")\n",
    "print(test_df['label'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nüìä Validation set labels:\")\n",
    "print(val_df['label'].value_counts().sort_index())\n",
    "\n",
    "# Plot label distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, (df, title) in enumerate([(train_df, 'Train'), (test_df, 'Test'), (val_df, 'Validation')]):\n",
    "    label_counts = df['label'].value_counts()\n",
    "    axes[idx].bar(label_counts.index, label_counts.values, color='steelblue')\n",
    "    axes[idx].set_title(f'{title} Set Label Distribution')\n",
    "    axes[idx].set_xlabel('Label')\n",
    "    axes[idx].set_ylabel('Count')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Label distribution visualized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fa6314",
   "metadata": {},
   "source": [
    "## Step 3: Analyze Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f0038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìä Train set shape: {train_df.shape}\")\n",
    "print(f\"üìä Test set shape: {test_df.shape}\")\n",
    "print(f\"üìä Validation set shape: {val_df.shape}\")\n",
    "\n",
    "total_samples = len(train_df) + len(test_df) + len(val_df)\n",
    "print(f\"\\nüìà Total samples: {total_samples:,}\")\n",
    "\n",
    "# Display column info\n",
    "print(\"\\nüìã Column Info:\")\n",
    "print(train_df.info())\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nüîç First 3 rows of training data:\")\n",
    "print(train_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d017e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available files in data directory\n",
    "print(\"üìÅ Files in data directory:\")\n",
    "for file in os.listdir('./data'):\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "# LIAR dataset column names\n",
    "columns = ['claim_id', 'claim', 'label', 'speaker', 'speaker_job_title', \n",
    "           'state_info', 'party_affiliation', 'barely_true_counts', \n",
    "           'false_counts', 'half_true_counts', 'mostly_true_counts', \n",
    "           'pants_on_fire_counts', 'context']\n",
    "\n",
    "# Load train, test, validation sets\n",
    "train_df = pd.read_csv('./data/train.tsv', sep='\\t', header=None, names=columns, on_bad_lines='skip')\n",
    "test_df = pd.read_csv('./data/test.tsv', sep='\\t', header=None, names=columns, on_bad_lines='skip')\n",
    "val_df = pd.read_csv('./data/val.tsv', sep='\\t', header=None, names=columns, on_bad_lines='skip')\n",
    "\n",
    "print(\"‚úÖ Datasets loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7818043",
   "metadata": {},
   "source": [
    "## Step 2: Load and Explore Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221278a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Colab: Download LIAR dataset from Kaggle\n",
    "import os\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "\n",
    "# Download LIAR dataset (requires Kaggle API setup)\n",
    "os.system('kaggle datasets download -d liar-dataset/liar-plus --unzip -p ./data')\n",
    "\n",
    "print(\"‚úÖ Dataset downloaded to ./data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63395b4",
   "metadata": {},
   "source": [
    "## Step 1: Download LIAR Dataset\n",
    "\n",
    "**Note for Colab**: Follow these steps:\n",
    "1. Go to https://www.kaggle.com/\n",
    "2. Create/login to account\n",
    "3. Go to Settings ‚Üí API ‚Üí \"Create New API Token\"\n",
    "4. Upload kaggle.json to Colab (or use !mkdir ~/.kaggle and paste credentials)\n",
    "5. Run cells below to download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e6233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60ca69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = ['pandas', 'numpy', 'matplotlib', 'seaborn', 'kaggle']\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"‚úÖ {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"üì¶ Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"‚úÖ {package} installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a5c819",
   "metadata": {},
   "source": [
    "# 01 - LIAR Dataset Exploration\n",
    "## Fake News Detection - Data Acquisition & EDA\n",
    "\n",
    "This notebook downloads and explores the LIAR dataset for fake news detection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
