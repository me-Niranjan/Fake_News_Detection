# Fake News Detection - ML Module

## Project Overview
This is the **ML component** of the Fake News Detection system. It handles:
- Data acquisition from LIAR dataset
- Text preprocessing and cleaning
- Feature extraction & embedding generation
- Training semantic similarity model
- Training NLI (Natural Language Inference) model
- Model evaluation and validation

## ğŸ“ Project Structure

```
ml_model/
â”œâ”€â”€ notebooks/                      # Jupyter notebooks for experimentation
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb        # LIAR dataset EDA
â”‚   â”œâ”€â”€ 02_data_preprocessing.ipynb      # Text cleaning & label conversion
â”‚   â”œâ”€â”€ 03_feature_extraction.ipynb      # Feature engineering
â”‚   â”œâ”€â”€ 04_semantic_similarity_model.ipynb
â”‚   â”œâ”€â”€ 05_nli_model_training.ipynb
â”‚   â”œâ”€â”€ 06_model_evaluation.ipynb
â”‚   â””â”€â”€ 07_full_pipeline_testing.ipynb
â”‚
â”œâ”€â”€ src/                            # Reusable Python modules
â”‚   â”œâ”€â”€ data_loader.py              # Load datasets
â”‚   â”œâ”€â”€ preprocessor.py             # Text preprocessing
â”‚   â”œâ”€â”€ utils.py                    # Utility functions
â”‚   â”œâ”€â”€ semantic_similarity.py       # Semantic model
â”‚   â”œâ”€â”€ nli_model.py                # NLI model
â”‚   â”œâ”€â”€ evidence_retriever.py        # Evidence retrieval
â”‚   â””â”€â”€ claim_verifier.py            # Main pipeline
â”‚
â”œâ”€â”€ data/                           # Raw datasets (from Kaggle)
â”‚   â”œâ”€â”€ train.tsv
â”‚   â”œâ”€â”€ test.tsv
â”‚   â””â”€â”€ val.tsv
â”‚
â”œâ”€â”€ preprocessed_data/              # Cleaned & processed data
â”‚   â”œâ”€â”€ train_data.csv
â”‚   â”œâ”€â”€ test_data.csv
â”‚   â”œâ”€â”€ val_data.csv
â”‚   â””â”€â”€ combined_data.csv
â”‚
â”œâ”€â”€ models/                         # Saved trained models
â”‚   â”œâ”€â”€ tokenizer.pkl
â”‚   â”œâ”€â”€ semantic_model.pth
â”‚   â”œâ”€â”€ nli_model.pth
â”‚   â””â”€â”€ embeddings.npy
â”‚
â”œâ”€â”€ configs/                        # Configuration files
â”‚   â”œâ”€â”€ model_config.json
â”‚   â”œâ”€â”€ paths_config.json
â”‚   â””â”€â”€ training_config.yaml
â”‚
â”œâ”€â”€ outputs/                        # Results & metrics
â”‚   â”œâ”€â”€ metrics.json
â”‚   â”œâ”€â”€ training_logs.txt
â”‚   â”œâ”€â”€ predictions.json
â”‚   â””â”€â”€ confusion_matrix.png
â”‚
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README.md                       # This file
```

## ğŸš€ Getting Started

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Download LIAR Dataset (for Colab)
- Go to https://www.kaggle.com/
- Download LIAR dataset: https://www.kaggle.com/datasets/liar-dataset/liar-plus
- Extract to `data/` folder

### 3. Run Notebooks in Order
1. **Notebook 01**: Data Exploration
2. **Notebook 02**: Data Preprocessing
3. **Notebook 03**: Feature Extraction
4. **Notebook 04**: Semantic Similarity Model
5. **Notebook 05**: NLI Model Training
6. **Notebook 06**: Model Evaluation
7. **Notebook 07**: Full Pipeline Testing

## ğŸ“Š Dataset Information

**LIAR Dataset:**
- **Size**: ~12,836 claims
- **Labels**: 6 â†’ Converted to 3
  - `true` + `mostly-true` â†’ **REAL**
  - `false` + `barely-true` + `pants-fire` â†’ **FAKE**
  - `half-true` â†’ **NOT_ENOUGH_INFO**

**Data Split:**
- Train: 10,269 samples
- Test: 1,284 samples
- Validation: 1,283 samples

## ğŸ”§ Key Features

### Data Preprocessing
- URL removal
- HTML tag removal
- Special character removal
- Lowercase conversion
- Whitespace normalization
- Label conversion (6 â†’ 3 classes)

### Models
1. **Semantic Similarity Model**
   - Matches claims with evidence
   - Uses sentence embeddings
   
2. **NLI Model**
   - Classifies claim-evidence pairs
   - Outputs: REAL / FAKE / NOT_ENOUGH_INFO

### Evaluation Metrics
- Accuracy
- Precision
- Recall
- F1-Score
- Confusion Matrix

## ğŸ“– Usage Example

```python
from src.data_loader import DataLoader
from src.preprocessor import TextPreprocessor

# Load data
loader = DataLoader()
train_df, test_df, val_df = loader.load_preprocessed_data()

# Preprocess new text
text = "Some claim text"
clean_text = TextPreprocessor.clean_text(text)
print(clean_text)
```

## ğŸ”— Integration with Backend

The trained models will be exported as:
- **Semantic Model**: `models/semantic_model.pth`
- **NLI Model**: `models/nli_model.pth`
- **Tokenizer**: `models/tokenizer.pkl`

Backend team will:
1. Load these models
2. Accept claims from frontend
3. Run through full pipeline
4. Return: Verdict + Confidence + Evidence

## ğŸ“ Next Steps

- [ ] Complete Notebook 01 - Data Exploration
- [ ] Complete Notebook 02 - Data Preprocessing
- [ ] Complete Notebook 03 - Feature Extraction
- [ ] Train Semantic Similarity Model
- [ ] Train NLI Model
- [ ] Evaluate Models
- [ ] Test Full Pipeline

## âš ï¸ Important Notes

- Run notebooks in **Google Colab** for GPU acceleration
- Upload Kaggle API token before running download
- Adjust batch size based on GPU memory
- Models will be saved automatically after training

## ğŸ“« Contact

**ML Person**: Niranjan S
- Handles: Data preparation, model training, evaluation

**Backend Person**: [TBD]
- Handles: Server setup, model integration

**Frontend Person**: [TBD]
- Handles: UI/UX, user input handling

---
**Version**: 1.0  
**Last Updated**: February 2026
